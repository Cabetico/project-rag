{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9f11dec-45c0-4475-aa7d-41674206c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1dbb0a-934d-4b37-a42d-ee5eef986ca5",
   "metadata": {},
   "source": [
    "### Retrieval evaluation                                                                                                                                                              \n",
    "The retrieval evaluation is about how good the search is performing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f198934b-8559-4aeb-8f63-4fe5e1703a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_question = pd.read_csv('../data/ground-truth-retrieval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8cb0fc6-9d30-4442-a760-a466f921a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = df_question.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2a0a6d2-c8ba-40b7-84e5-b811e24e1ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'question': \"What do I need to do after clicking the 'Sign Up' button?\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a235c8e-f390-4a1a-8950-57130bf0c14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "    return total_score / len(relevance_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d5a02ef-80e9-4bd6-827e-6d051bcc8e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d805df65-dca8-42ad-9705-2e76436487f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a8b5843-c4b0-40a5-9d4f-81b071b9fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qdrant_search(query, limit=5):\n",
    "\n",
    "    collection_name=\"project\"\n",
    "    model_handle = \"jinaai/jina-embeddings-v2-small-en\"\n",
    "    \n",
    "    result_points = qdrant_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=models.Document( #embed the query text locally with \"jinaai/jina-embeddings-v2-small-en\"\n",
    "            text=query,\n",
    "            model=model_handle \n",
    "        ),\n",
    "        limit=limit, # top closest matches\n",
    "        with_payload=True #to get metadata in the results\n",
    "    )\n",
    "\n",
    "    results = [point.payload for point in result_points.points]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a0ba7a8-0a1b-4644-a9a8-9870e81781ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf_search(query: str, limit: int = 1) -> list[models.ScoredPoint]:\n",
    "    result_points = qdrant_client.query_points(\n",
    "        collection_name=\"project-sparse-and-dense\",\n",
    "        prefetch=[\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=\"jinaai/jina-embeddings-v2-small-en\",\n",
    "                ),\n",
    "                using=\"jina-small\",\n",
    "                limit=(3 * limit),\n",
    "            ),\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=\"Qdrant/bm25\",\n",
    "                ),\n",
    "                using=\"bm25\",\n",
    "                limit=(3 * limit),\n",
    "            ),\n",
    "        ],\n",
    "        # Fusion query enables fusion on the prefetched results\n",
    "        query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "        with_payload=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    results = [point.payload for point in result_points.points]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c3770f7-d42f-47dc-9e79-4fb7f30bdc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_total = []\n",
    "doc_id = q['id']\n",
    "results = qdrant_search(q['question'])\n",
    "relevance = [d['id'] == doc_id for d in results]\n",
    "relevance_total.append(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1139cfbc-33ab-481c-8f50-62763217d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['id']\n",
    "        results = search_function(q['question'])\n",
    "        relevance = [d['id'] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f94e0eb6-603f-4573-af09-ddf8d7cccdd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d666612cbe4b7ea982d9195f8cbe46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.9063291139240506, 'mrr': 0.7727004219409283}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ground_truth, rrf_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33327e71-9680-4cca-85c3-790746de6b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client_openai = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f34c3e-d7f2-44d7-af2a-9d4e9d0f1217",
   "metadata": {},
   "source": [
    "### LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a10c7c0a-cfce-49b0-b959-b3fd5871e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt_template = \"\"\"\n",
    "You are an expert evaluator for a RAG system.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8222802e-c883-4d80-a721-fefe035cf42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "            You're are the customer service chatbot of an e-commerce platform. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "            Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "            \n",
    "            QUESTION: {question}\n",
    "            \n",
    "            CONTEXT: \n",
    "            {context}\n",
    "            \"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"prompt: {doc['prompt']}\\nresponse: {doc['response']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "790e0976-282f-4d6a-8bd3-bd4078b06deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt, model=\"gpt-4o-mini\"):\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "41c9ecb8-ec8c-4bb4-a670-037f7adf9566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_relevance(question, answer):\n",
    "    prompt = evaluation_prompt_template.format(question=question, answer=answer)\n",
    "    evaluation = llm(prompt, model=\"gpt-4o-mini\")\n",
    "\n",
    "    try:\n",
    "        json_eval = json.loads(evaluation)\n",
    "        return json_eval\n",
    "    except json.JSONDecodeError:\n",
    "        result = {\"Relevance\": \"UNKNOWN\", \"Explanation\": \"Failed to parse evaluation\"}\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8467fa9-336b-4226-80a2-fe6feb7c5dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You can find the 'Sign Up' button on the top right corner of our website.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c63a2359-e5c4-4780-8485-be73e88d667d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Where can I find the 'Sign Up' button on your website?\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "789ac486-5caa-4764-b9e3-244b9b694e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random = df_question.groupby(\"id\", group_keys=False).sample(1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7d578915-c8d4-41cb-a7eb-d0020ecd7007",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "questions = []\n",
    "answers = [] \n",
    "relevances = []\n",
    "for row in df_random.to_dict(orient='records'):\n",
    "    ids.append(row['id'])\n",
    "    questions.append(row['question'])\n",
    "    answer = llm(build_prompt(row['question'], rrf_search(row['question'])))\n",
    "    answers.append(answer)\n",
    "    relevances.append(evaluate_relevance(row['question'], answer)['Relevance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d129804d-3ae5-46dd-84a4-512a310d4d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relevance = pd.DataFrame({'id': ids,\n",
    "             'question': questions,\n",
    "             'answer': answers,\n",
    "             'relevance': relevances\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c9982a-c87d-41d9-8c46-3f933de38bfb",
   "metadata": {},
   "source": [
    "### RELEVANCE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2421ff5e-3c48-4eb4-aa8b-0c08fcb5f9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relevance.to_csv('../data/rag-eval.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3b7b0916-fc37-457f-832c-6f05a84328eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8734177215189873"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relevance.query(\"relevance == 'RELEVANT'\").shape[0] / df_relevance.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
